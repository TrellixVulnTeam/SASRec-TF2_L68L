{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from sasrec import SASREC\n",
    "from sasrec_text import SASREC_TEXT\n",
    "from ssept import SSEPT\n",
    "from util import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(dataset, text_maxlen, vocab_size, text_embed):\n",
    "    data_dir = '../data/'\n",
    "    filename = dataset + \"_item_description.txt\"\n",
    "    glove_dir = \"/recsys_data/datasets/glove\"\n",
    "    glove_file = 'glove.6B.50d.txt'\n",
    "    maxlen = text_maxlen\n",
    "    vocab_size = vocab_size\n",
    "    embedding_dim = text_embed\n",
    "\n",
    "    print(f\"Processing for textual features\")\n",
    "    with open(os.path.join(data_dir, filename), 'r') as fr:\n",
    "        docs = fr.readlines()\n",
    "    tokenizer = Tokenizer(num_words=vocab_size-1, lower=True, split=' ')  # 1 ... 4999\n",
    "    # tokenizer = Tokenizer(num_words=vocab_size, lower=True, split=' ', oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    print(f\"Number of words found: {len(tokenizer.word_index)}\")\n",
    "    vocab = [k for k,v in tokenizer.word_index.items() if v < vocab_size]  # 1 ... 4999\n",
    "    tensor = tokenizer.texts_to_sequences(docs)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=maxlen)\n",
    "    print(f\"Tokenized each item description\", tensor.shape)\n",
    "\n",
    "    # add a zero row\n",
    "    num_items, seq_len = tensor.shape\n",
    "    big_tensor = np.zeros((num_items+1, seq_len))\n",
    "    big_tensor[1:num_items+1, :] = tensor\n",
    "\n",
    "    embedding_matrix, glove_vocab = create_embedding_matrix(os.path.join(glove_dir, glove_file),\n",
    "                                                            vocab,  \n",
    "                                                            embedding_dim,\n",
    "                                                            vocab_size)\n",
    "    item_embeddings = np.zeros((num_items+1, embedding_matrix.shape[1]))\n",
    "    for item in tqdm(range(1, num_items+1)):\n",
    "        word_indices = big_tensor[item, :]\n",
    "        word_indices = [int(i) for i in word_indices if i != 0]\n",
    "        if len(word_indices) > 0:\n",
    "            word_vectors = embedding_matrix[word_indices, :]\n",
    "            mean_vector = word_vectors.mean(axis=0)\n",
    "            item_embeddings[item,:] = mean_vector\n",
    "        else:\n",
    "            print(f\"Missing embedding for item-{item}\")\n",
    "            print(f\"{item}-text: {docs[item-1]}\")\n",
    "\n",
    "    print(f\"Text based item embedding matrix\", item_embeddings.shape)\n",
    "    return item_embeddings, embedding_matrix, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemnum = 67310\n",
    "maxlen = 50\n",
    "num_blocks = 2\n",
    "hidden_units = 100\n",
    "num_heads = 1\n",
    "dropout_rate = 0.5\n",
    "l2_emb = 0.0\n",
    "num_neg_test = 100\n",
    "\n",
    "dataset = 'Beauty'\n",
    "text_maxlen, vocab_size, text_embed = 100, 5000, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for textual features\n",
      "Number of words found: 68940\n",
      "Tokenized each item description (67310, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1281/67310 [00:00<00:05, 12799.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! 239 words could not be mapped\n",
      "Missing embedding for item-929\n",
      "929-text: Revivasol\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 6269/67310 [00:00<00:04, 12599.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing embedding for item-4107\n",
      "4107-text: Raydiant\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 29520/67310 [00:02<00:02, 12662.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing embedding for item-27989\n",
      "27989-text: Accentus: Transcriptions\n",
      "\n",
      "Missing embedding for item-29688\n",
      "29688-text: Euc-4338\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 34670/67310 [00:02<00:02, 12827.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing embedding for item-33371\n",
      "33371-text: Mavala Scientifique\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 53024/67310 [00:04<00:01, 13215.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing embedding for item-50757\n",
      "50757-text: Ponybun\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67310/67310 [00:05<00:00, 12848.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing embedding for item-65491\n",
      "65491-text: Magick\n",
      "\n",
      "Text based item embedding matrix (67311, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embed_matrix, word_embedddings, docs = text_processing(dataset, text_maxlen, vocab_size, text_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67311, 50) (5000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(embed_matrix.shape, word_embedddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.4820001125335693, 3.955150008201599)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_matrix.min(), embed_matrix.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.940700054168701, 4.365699768066406)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedddings.min(), word_embedddings.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = DistanceMetric.get_metric('euclidean')\n",
    "dm = dist.pairwise(embed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67311/67311 [06:25<00:00, 174.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# dm_s = np.argsort(dm, axis=1)\n",
    "K = 5\n",
    "similar_items = np.zeros((itemnum+1, K))\n",
    "for ii in tqdm(range(dm.shape[0])):\n",
    "    vec = dm[ii, :]\n",
    "    indx = np.argsort(vec)[:(K+1)]\n",
    "    similar_items[ii, :] = [int(jj) for jj in indx if jj != ii][:K]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similar_items(item, similar_items, docs):\n",
    "    sitems = similar_items[item, :]\n",
    "    print(docs[item-1])\n",
    "    print(\"*************similar items**************\")\n",
    "    for it in sitems:\n",
    "        print(docs[int(it-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c. Booth derma M 36 Oxygen Infusion Cell Rejuvenation Peel Kit 1 kit Oxygen Infusion Cell Rejuvenation Peel KitA powerful Two-Step SystemErases dead cells so skin can breatheReverses sun damage and premature agingImmediate, breath-taking resultsYou feel sluggish without enough oxygen to breathe. So does your skin. So peel away the dead, dry, flaky skin cells that deprive your skin of oxygen. When every pore breathes freely, skin comes alive with renewed vibrancy. Fine lines virtually disappear. Sun damage is reversed. Breakouts? A thing of the past. Best of all, results are immediate and remarkable. Because a little oxygen goes a\n",
      "\n",
      "*************similar items**************\n",
      "Yes To Tomatoes Skin Clearing Facial Mask, 1.7 Fluid Ounce A skin clearing facial mask that not only absorbs sebum buildup and keeps pores clear, but also contains dead sea mud to exfoliate your skin and encourage a shine-free appearance. Our deep pore treatment contains the potent antioxidant lycopene from organic tomatoes to brighten, refine and purify the skin. Now that&#x2019;s something to say yes about.\n",
      "\n",
      "OBAGI Eye Treatment Gel 0.5 oz (15 g) Finally -- an eye treatment that really works to bring back the lift around your eyes. Obagi Elastiderm Eye Products are one-of-a-kind eye treatments that include a revolutionary bi-mineral complex, copper zinc malonate, to help restore the elasticity around your eyes and reduce the appearance of visible fine lines and wrinkles. Obagi Elastiderm Eye Treatment Gel is a cool, fluid gel for normal to oily skin. Elastiderm Eye Treatment Gel is ideal if you live in a tropical or humid climate. Elastiderm Eye Products contain powerful, yet gentle ingredients, including blueberry extract,\n",
      "\n",
      "1/2 Oz Bottle of Ocean Power Booster Drops for Intensifing Your Sunless Spray Tan Ocean Power Booster Drops are more than just a bronzing boost in your DHA solution, they contain copper aminos that assist in naturally building collagen and help firm the skin. Our formula also contains a glycolic ester designed to exfoliate your skin in unique way to allow the bronzing element to absorb into your skin, resulting in a longer lasting bronze color that will not wash your tanning investment down the drain. Ocean Power Booster will hydrate and soften you skin for a luxurious silky feel\n",
      "\n",
      "Gillette Fusion ProSeries Face Scrub, Thermal, 3.3 oz. Introducing a men's facial scrub that warms, deep cleans and helps soften facial hair before shaving. In this formula, heating technology is activated on contact with water to provide warming comfort. A mild soap-free system cleans skin by removing oils (sebum) and dirt while helping to soften hair, making it easier to cut. Small particles then help exfoliate dead, dry skin cells - providing an even, clear path for the razor. Simply feel the warm sensation for the very first time and you'll know that this is the face scrub for you.\n",
      "\n",
      "The Body Shop Bath Lily, Coral Pink Best if you want to: Increase the lather of your favorite shower gel with a lovely bath lily that gently exfoliates to remove dead skin cells. How it works: Suitable for all skin types, particularly dry skin that needs regular exfoliation. Product lathering benefits mean you only need a small quantity of shower gel each time, so your product will last longer. Prepares skin to better absorb moisturizer by removing layer of dead skin cells....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_similar_items(20000, similar_items, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Revivasol\\n', 'Raydiant\\n', 'Accentus: Transcriptions\\n')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[928], docs[4106], docs[27988]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url(url='http://snap.stanford.edu/graphsage/reddit.zip', \n",
    "             save_path='/recsys_data/GraphSAGE/example_data/reddit.zip', chunk_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SASREC_TEXT(item_num=itemnum,\n",
    "            seq_max_len=maxlen,\n",
    "            num_blocks=num_blocks,\n",
    "            embedding_dim=hidden_units,\n",
    "            attention_dim=hidden_units,\n",
    "            attention_num_heads=num_heads,\n",
    "            dropout_rate=dropout_rate,\n",
    "            l2_reg=l2_emb,\n",
    "            num_neg_test=num_neg_test,\n",
    "#             max_seq_len_text=text_maxlen,\n",
    "#             vocab_size=vocab_size,\n",
    "            text_embedding_dimension=text_embed,\n",
    "            item_text_embedding_matrix=embed_matrix,\n",
    "#             item_text_sequences=item_desc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fabb7e99c90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../sample_nan.pkl', 'rb') as fr:\n",
    "    inputs = pickle.load(fr)\n",
    "    \n",
    "model.load_weights('../checkpoints/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(50, 100) dtype=float32, numpy=\n",
       " array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(50, 100) dtype=float32, numpy=\n",
       " array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(50, 100) dtype=float32, numpy=\n",
       " array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(50, 100) dtype=float32, numpy=\n",
       " array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(50, 100) dtype=float32, numpy=\n",
       " array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(50, 100) dtype=float32, numpy=\n",
       " array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f9433ad0690>\n",
      "item_embeddings <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f94302a6410>\n",
      "positional_embeddings <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f94306a37d0>\n",
      "dropout <tensorflow.python.keras.layers.core.Dropout object at 0x7f9433ac0b90>\n",
      "encoder <sasrec_text.Encoder object at 0x7f9433ac0610>\n",
      "masking <tensorflow.python.keras.layers.core.Masking object at 0x7f9433ac0f50>\n",
      "layer_normalization_6 <sasrec_text.LayerNormalization object at 0x7f9433ac0e90>\n",
      "text_encoder <sasrec_text.TextEncoder object at 0x7f942ed79f90>\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weights)\n",
    "# print(model.layers[0].bias.numpy())\n",
    "# print(model.layers[0].bias_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x7f9433ad0690>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['users', 'input_seq', 'positive', 'negative'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35322])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['users'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "       13599, 20493,  8465, 50130,  9629], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_seq'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "       20493,  8465, 50130,  9629, 60954], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['positive'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "       23654, 15273, 41160, 36779, 40662], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['negative'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 50)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.417e+03, 2.850e+02, 6.000e+00, 2.320e+02, 1.240e+02, 1.200e+03,\n",
       "       9.270e+02, 2.200e+01, 1.330e+02, 1.161e+03, 1.400e+01, 3.619e+03,\n",
       "       1.240e+02, 1.600e+01, 3.500e+01, 5.700e+01, 2.850e+02, 1.500e+01,\n",
       "       6.000e+01, 1.019e+03, 5.020e+02, 2.460e+02, 1.000e+01, 6.000e+00,\n",
       "       1.200e+02, 3.917e+03, 1.287e+03, 3.000e+00, 1.507e+03, 2.000e+00,\n",
       "       1.950e+02, 4.000e+00, 1.103e+03, 2.768e+03, 1.400e+01, 2.200e+01,\n",
       "       3.000e+00, 3.552e+03, 1.540e+02, 1.446e+03, 4.370e+02, 2.000e+00,\n",
       "       2.320e+02, 2.100e+01, 9.300e+01, 2.000e+00, 1.821e+03, 1.000e+00,\n",
       "       4.050e+02, 1.220e+02, 2.720e+02, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "       0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "       0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "       0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "       0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "       0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "       0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "       0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "       0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_desc[5265,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
