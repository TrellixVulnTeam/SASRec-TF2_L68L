{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e5353cd",
   "metadata": {},
   "source": [
    "## Build Input for Transformer Based Recommender with Adjacency Data\n",
    "\n",
    "SASRec [1] takes sequence of items (interacted by a user) as input and predicts the same sequence (shifted). The idea is to enrich this item sequence with additional information coming from the associated users. Thus, each item will have a sequence \n",
    "of users (sorted by time) as additional attributes. \n",
    "\n",
    "How to take care of the time aspect? While creating this graph only the users interacted before this time should be taken into account. Thus, the new dataset will look like \n",
    "        \n",
    "        user-id, item-id, all the users who interacted this item recently    \n",
    "        \n",
    "        11 56076 0\n",
    "        11 14037 15,10,12,13\n",
    "        11 4467 0\n",
    "        11 33810 268,41162,60222,56206,49801,10441,13000,14299\n",
    "        11 31260 12817,14614,30088,11039,25632,13,62373,47260,45849\n",
    "        11 28006 32489\n",
    "        11 16413 11359,11315,14025,34607,41079,11448,41139,40790,2541,10873,41072,41089,41083,13000,41099,29498,26935,9951,41060\n",
    "        11 55039 59475,11315,14025,34607,11448,61040,41089,41099,41083,2541,13407,20417,9951\n",
    "        11 20799 56213,56198\n",
    "        11 58690 3616\n",
    "        11 26147 0\n",
    "        11 66039 51660,52634,58450,51306,59567,10873\n",
    "        11 78708 58957\n",
    "        11 18158 3597,951,61249,2901,40226,60070,32243,32556,3635"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eba7fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict, Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5e718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_filename = \"/recsys_data/RecSys/SASRec-tf2/data/ae_original.txt\"\n",
    "output_filename = \"/recsys_data/RecSys/SASRec-tf2/data/ae_graph.txt\"\n",
    "dict_filename = \"/recsys_data/RecSys/SASRec-tf2/data/ae_graph_dict.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0197567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process_with_time(fname, pname, sep=\"\\t\", file_write=False, max_seq_len=50):\n",
    "    User = defaultdict(list)\n",
    "    Items = set()\n",
    "    user_dict, item_dict = {}, {}\n",
    "    item_user = defaultdict(list)  # track user interaction time\n",
    "    final_item_user = dict()\n",
    "\n",
    "    with open(fname, 'r') as fr:\n",
    "        for line in fr:\n",
    "            u, i, t = line.rstrip().split(sep)\n",
    "            t = float(t)\n",
    "            User[u].append((i, t))\n",
    "            Items.add(i)\n",
    "            item_user[i].append((u, t))\n",
    "            \n",
    "    print(len(User), len(Items))\n",
    "    \n",
    "    item_count = 1  # always start with 1\n",
    "    for item in Items:\n",
    "        item_dict[item] = item_count\n",
    "        item_count += 1\n",
    "\n",
    "    count_del = 0\n",
    "    user_count = 1  # start with 1\n",
    "\n",
    "    # get the user-ids\n",
    "    for user in User.keys():\n",
    "        if len(User[user]) <= 2:\n",
    "            count_del += 1\n",
    "        else:\n",
    "            User[user] = sorted(User[user], key=lambda x: x[1])\n",
    "            user_dict[user] = user_count\n",
    "            user_count += 1\n",
    "\n",
    "    if file_write:\n",
    "        print(f\"Writing data in {pname}\")\n",
    "        count_missing = 0\n",
    "        with open(pname, 'w') as fw:\n",
    "            for user in tqdm(User.keys()):\n",
    "                if len(User[user]) > 2:\n",
    "                    items = sorted(User[user], key=lambda x: x[1])\n",
    "                    user_id = user_dict[user]\n",
    "                    missing_user = 0\n",
    "                    for it in items:\n",
    "                        item_name, item_time = it\n",
    "                        ut = item_user[item_name]\n",
    "                        item_id = item_dict[item_name]\n",
    "                        prev_ut = [x for x in ut if x[1] < item_time]\n",
    "                        prev_ut = sorted(prev_ut, key=lambda x: item_time - x[1])\n",
    "                        prev_u = [str(user_dict[x[0]]) for x in prev_ut if x[0] in user_dict][:max_seq_len]\n",
    "                        if len(prev_u) == 0:\n",
    "                            hist = '0'\n",
    "                            missing_user += 1\n",
    "                        else:\n",
    "                            hist = ','.join(prev_u)\n",
    "                        fw.write(sep.join([str(user_id), str(item_id), hist]) + '\\n')\n",
    "                    if missing_user == len(items):\n",
    "                        count_missing += 1\n",
    "        \n",
    "    print(user_count-1, count_del, count_missing)\n",
    "    return user_dict, item_dict, User, item_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c8efc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63161 85930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 45/63161 [00:00<02:20, 448.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data in /recsys_data/RecSys/SASRec-tf2/data/ae_graph.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63161/63161 [00:52<00:00, 1194.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63114 47 12\n",
      "Retained 63114 users with 85930 items from 63161 users\n"
     ]
    }
   ],
   "source": [
    "write_file = True\n",
    "max_user_list = 50\n",
    "udict, idict, user_history, item_history = data_process_with_time(interaction_filename, \n",
    "                                                                  output_filename, \n",
    "                                                                  \"\\t\", \n",
    "                                                                  write_file,\n",
    "                                                                  max_user_list\n",
    "                                                                 )\n",
    "\n",
    "if write_file:\n",
    "    with open(dict_filename, 'wb') as handle:\n",
    "        pickle.dump((udict, idict, user_history), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"Retained {len(udict)} users with {len(idict)} items from {len(user_history)} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2db11e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:azureml_py36_automl]",
   "language": "python",
   "name": "conda-env-azureml_py36_automl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
