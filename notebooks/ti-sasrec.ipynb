{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Interval Based Transformer Model\n",
    "\n",
    "The original Transformer based recommender does not take into account of the time interval between two successive interactions. The paper Time Interval Aware Self-Attention for Sequential Recommendation, Jiacheng Li, Yujie Wang, Julian McAuley, WSDM, 2020 introduced the logic of including the time information. \n",
    "\n",
    "The original Git repo with TF 1.x is https://github.com/JiachengLi1995/TiSASRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'retrying'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-65e4b031d13d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/recsys_data/RecSys/SASRec-tf2/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_and_process_amazon\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/recsys_data/RecSys/SASRec-tf2/download_and_process_amazon.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtempfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemporaryDirectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mretrying\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'retrying'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "sys.path.insert(0, \"/recsys_data/RecSys/SASRec-tf2/\")\n",
    "\n",
    "import download_and_process_amazon as dpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/recsys_data/RecSys/SASRec-tf2/data\"\n",
    "meta_filename = 'meta_Electronics.json'\n",
    "encoded_file = \"ae_v3.txt\"\n",
    "\n",
    "# 5-core\n",
    "category = \"Electronics\"\n",
    "download_url = f\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_{category}_5.json.gz\"\n",
    "reviews_name = f\"reviews_{category}_5.json\"\n",
    "reviews_file = os.path.join(data_dir, reviews_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data for ***Electronics***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/recsys_data/RecSys/SASRec-tf2/data/reviews_Electronics_5.json'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Generating data for ***{category}***\")\n",
    "dpa.download_and_extract(reviews_name, reviews_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start reviews preprocessing...\n",
      "Processed data in /recsys_data/RecSys/SASRec-tf2/data/reviews_Electronics_5.json_output\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(reviews_file + '_output'):\n",
    "    reviews_output_file = dpa._reviews_preprocessing(reviews_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process_with_time(fname, pname, K=3, sep=\"\\t\", file_write=False, add_time=False):\n",
    "    User = defaultdict(list)\n",
    "    Items = set()\n",
    "    user_dict, item_dict = {}, {}\n",
    "\n",
    "    with open(fname, 'r') as fr:\n",
    "        for line in fr:\n",
    "            u, i, t = line.rstrip().split(sep)\n",
    "            User[u].append((i, t))\n",
    "            Items.add(i)\n",
    "    \n",
    "    print(len(User), len(Items))\n",
    "    item_count = 1\n",
    "    for item in Items:\n",
    "        item_dict[item] = item_count\n",
    "        item_count += 1\n",
    "\n",
    "    count_del = 0\n",
    "    user_count = 1\n",
    "    if file_write:\n",
    "        print(f\"Writing data in {pname}\")\n",
    "        with open(pname, 'w') as fw:\n",
    "            for user in User.keys():\n",
    "                if len(User[user]) < K:\n",
    "#                     del User[user]\n",
    "                    count_del += 1\n",
    "                else:\n",
    "                    # user_dict[user] = user_count\n",
    "                    items = sorted(User[user], key=lambda x: x[1])\n",
    "                    timestamps = [x[1] for x in items]\n",
    "                    items = [item_dict[x[0]] for x in items]\n",
    "                    for i, t in zip(items, timestamps):\n",
    "                        out_txt = [str(user_count), str(i)]\n",
    "                        if add_time:\n",
    "                            out_txt.append(str(t))\n",
    "                        fw.write(sep.join(out_txt) + \"\\n\")\n",
    "                    user_dict[user] = user_count\n",
    "                    user_count += 1\n",
    "    else:\n",
    "        for user in User.keys():\n",
    "            if len(User[user]) < K:\n",
    "                # del User[user]\n",
    "                count_del += 1\n",
    "            else:\n",
    "                User[user] = sorted(User[user], key=lambda x: x[1])\n",
    "                user_dict[user] = user_count\n",
    "                user_count += 1\n",
    "        \n",
    "    print(user_count-1, count_del)\n",
    "    return user_dict, item_dict, User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 192403 users and 63001 items\n",
      "Total 192403 users and 63001 items\n",
      "Total 192403 users, 0 removed\n",
      "Processed model input data in /recsys_data/RecSys/SASRec-tf2/data/ae_v3.txt\n"
     ]
    }
   ],
   "source": [
    "udict, idict = dpa.data_process_with_time(reviews_output_file,\n",
    "                                      os.path.join(data_dir, encoded_file),\n",
    "                                      K=5,\n",
    "                                      sep=\"\\t\",\n",
    "                                      item_set=None,\n",
    "                                      add_time=True)\n",
    "len(udict), len(idict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 63161 users and 85930 items\n",
      "27773 items have less than 3 interactions\n",
      "47 users have less than 3 interactions\n",
      "Total 63114 users and 58157 items\n",
      "Total 63073 users, 41 removed\n",
      "Processed model input data in /recsys_data/RecSys/SASRec-tf2/data/ae_v3.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(63073, 58157)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udict, idict = dpa.data_process_with_time(os.path.join(data_dir, \"ae_original.txt\"),\n",
    "                                          os.path.join(data_dir, encoded_file),\n",
    "                                          K=3,\n",
    "                                          sep=\"\\t\",\n",
    "                                          item_set=None,\n",
    "                                          add_time=True)\n",
    "len(udict), len(idict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63161 85930\n",
      "Writing data in /recsys_data/RecSys/SASRec-tf2/data/ae_v3.txt\n",
      "63114 47\n",
      "Retained 63114 users with 85930 items from 63161 users\n"
     ]
    }
   ],
   "source": [
    "udict, idict, user_history = data_process_with_time(os.path.join(data_dir, \"ae_original.txt\"),\n",
    "                                                    os.path.join(data_dir, encoded_file),\n",
    "                                                    K=3,\n",
    "                                                    sep=\"\\t\",\n",
    "                                                    file_write=True,\n",
    "                                                    add_time=True)\n",
    "print(f\"Retained {len(udict)} users with {len(idict)} items from {len(user_history)} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n"
     ]
    }
   ],
   "source": [
    "User = dpa.data_partition(os.path.join(data_dir, \"ae_v3.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[71865, 1276992000.0],\n",
       " [73699, 1295568000.0],\n",
       " [76752, 1305504000.0],\n",
       " [70038, 1339632000.0],\n",
       " [52031, 1369440000.0],\n",
       " [5655, 1370736000.0],\n",
       " [67712, 1370736000.0],\n",
       " [36497, 1382659200.0],\n",
       " [54084, 1390176000.0],\n",
       " [76563, 1390176000.0],\n",
       " [58972, 1390176000.0],\n",
       " [26213, 1390176000.0],\n",
       " [62645, 1390176000.0],\n",
       " [39023, 1392076800.0],\n",
       " [49569, 1393113600.0],\n",
       " [11443, 1395187200.0],\n",
       " [83584, 1395878400.0],\n",
       " [38275, 1403740800.0]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "User[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Preparing done...\n"
     ]
    }
   ],
   "source": [
    "[user_train, user_valid, user_test, usernum, itemnum, timenum] = dpa.data_partition(os.path.join(data_dir, \"ae_v3.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[33306, 1],\n",
       " [34165, 28],\n",
       " [35596, 42],\n",
       " [32497, 92],\n",
       " [24147, 135],\n",
       " [2640, 137],\n",
       " [31413, 137],\n",
       " [16854, 154],\n",
       " [25087, 165],\n",
       " [35505, 165],\n",
       " [27370, 165],\n",
       " [12196, 165],\n",
       " [29087, 165],\n",
       " [18026, 167],\n",
       " [23003, 169],\n",
       " [5337, 172]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18576000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1295568000 - 1276992000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps, n_features = 96, 6\n",
    "in1 = tf.keras.Input(shape=(n_timesteps, n_features))\n",
    "conv1 = tf.keras.layers.Conv1D(2, 2, strides=1)(in1)\n",
    "model = tf.keras.Model(inputs=in1, outputs=conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 96, 6)]           0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 95, 2)             26        \n",
      "=================================================================\n",
      "Total params: 26\n",
      "Trainable params: 26\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv1d/kernel:0' shape=(2, 6, 2) dtype=float32, numpy=\n",
       " array([[[ 0.30539536,  0.32176018],\n",
       "         [-0.24371243,  0.42594963],\n",
       "         [-0.28002533, -0.2270734 ],\n",
       "         [-0.21885604, -0.07609427],\n",
       "         [ 0.14368159, -0.5911813 ],\n",
       "         [ 0.51014   ,  0.13580382]],\n",
       " \n",
       "        [[ 0.27753323, -0.5951712 ],\n",
       "         [-0.29096928,  0.05894744],\n",
       "         [-0.31501025,  0.29697037],\n",
       "         [ 0.08256078,  0.20293069],\n",
       "         [-0.45090458, -0.5907528 ],\n",
       "         [ 0.04409683,  0.6015304 ]]], dtype=float32)>,\n",
       " <tf.Variable 'conv1d/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w - grad * lr\n",
    "grad = average(grad_i), i = 1.,, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLSTMCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_dim):\n",
    "        super(myLSTMCell, self).__init__()\n",
    "        self.big_matrix = tf.keras.layers.Dense(\n",
    "                            units=4*out_dim, activation=None, use_bias=True,\n",
    "                            kernel_initializer='glorot_uniform',\n",
    "                            bias_initializer='zeros', kernel_regularizer=None,\n",
    "                            bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "                            bias_constraint=None\n",
    "                        )\n",
    "        self.sigmoid = tf.math.sigmoid\n",
    "        self.phi = tf.math.tanh\n",
    "        \n",
    "    def call(self, x, states):\n",
    "        h, c = states\n",
    "        all_y = self.big_matrix(tf.concat([x, h], axis=-1))\n",
    "        ys = tf.split(all_y, 4, axis=-1)\n",
    "        fk = self.sigmoid(ys[0])\n",
    "        ik = self.sigmoid(ys[1])\n",
    "        ck = tf.math.multiply(fk, c) + tf.math.multiply(ik, self.phi(ys[2]))\n",
    "        ok = self.sigmoid(ys[3])\n",
    "        hk = tf.math.multiply(ok, self.phi(ck))\n",
    "        return ok, (hk, ck)\n",
    "    \n",
    "class myLSTM(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_dim, return_sequences=False, return_state=False):\n",
    "        super(myLSTM, self).__init__()\n",
    "        self.cell = myLSTMCell(out_dim)\n",
    "        self.out_dim = out_dim\n",
    "        self.return_sequences = return_sequences\n",
    "        self.return_state = return_state\n",
    "        \n",
    "    def call(self, x):\n",
    "        b, s, h = x.shape\n",
    "        prev_state = (tf.random.normal([b, h], 0, 1, tf.float32), \n",
    "                      tf.random.normal([b, h], 0, 1, tf.float32))\n",
    "\n",
    "        if self.return_sequences:\n",
    "            all_ys =[]\n",
    "            if self.return_state:\n",
    "                all_hs = []\n",
    "                all_cs = []\n",
    "                \n",
    "        for ii in range(s):\n",
    "            x_i = x[:, ii, :]\n",
    "            y_i, new_state = self.cell(x_i, prev_state)\n",
    "            if self.return_sequences:\n",
    "                all_ys.append(y_i)\n",
    "                if self.return_state:\n",
    "                    all_hs.append(new_state[0])\n",
    "                    all_cs.append(new_state[1])\n",
    "            prev_state = new_state\n",
    "            \n",
    "        if self.return_sequences:\n",
    "            if self.return_state:\n",
    "                return all_ys, (all_hs, all_cs)\n",
    "            else:\n",
    "                return all_ys\n",
    "        else:\n",
    "            if self.return_state:\n",
    "                return y_i, new_state\n",
    "            else:\n",
    "                return y_i\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim, out_dim, seq_len = 32, 32, 10\n",
    "batch_dim = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(5)\n",
    "x = tf.random.normal([batch_dim, seq_len, in_dim], 0, 1, tf.float32)\n",
    "lstm = myLSTM(out_dim)\n",
    "y = lstm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 32), dtype=float32, numpy=\n",
       "array([[0.5599368 , 0.23224984, 0.49254808, 0.5926059 , 0.5832714 ,\n",
       "        0.5794184 , 0.5536603 , 0.44985098, 0.3392653 , 0.4666337 ,\n",
       "        0.5440046 , 0.51614356, 0.52931595, 0.48723206, 0.45453623,\n",
       "        0.5113864 , 0.60725   , 0.5892867 , 0.46545208, 0.5244474 ,\n",
       "        0.5753214 , 0.4265751 , 0.6043889 , 0.6281752 , 0.47321272,\n",
       "        0.34909117, 0.34366786, 0.65943074, 0.3755182 , 0.27139494,\n",
       "        0.6502329 , 0.47100565],\n",
       "       [0.30076072, 0.63117933, 0.38542297, 0.43152928, 0.64638555,\n",
       "        0.49973753, 0.66102844, 0.52569866, 0.42737994, 0.51383024,\n",
       "        0.3163207 , 0.5394806 , 0.35988444, 0.76918477, 0.33002552,\n",
       "        0.44780493, 0.41091138, 0.5326197 , 0.33237004, 0.5004793 ,\n",
       "        0.40655237, 0.68841195, 0.3764247 , 0.30550352, 0.7358771 ,\n",
       "        0.6590759 , 0.37678885, 0.45530462, 0.5368906 , 0.3561387 ,\n",
       "        0.49229378, 0.3945621 ],\n",
       "       [0.38124165, 0.6049407 , 0.6947774 , 0.62442875, 0.5124052 ,\n",
       "        0.54766756, 0.58953625, 0.5730521 , 0.4132718 , 0.5558802 ,\n",
       "        0.16355515, 0.51638997, 0.29966345, 0.4801739 , 0.4794883 ,\n",
       "        0.5684878 , 0.48225713, 0.14647445, 0.4241309 , 0.5894765 ,\n",
       "        0.3781194 , 0.7395627 , 0.4868408 , 0.50109875, 0.3734831 ,\n",
       "        0.6972993 , 0.47589278, 0.46482715, 0.7394978 , 0.80157316,\n",
       "        0.5666607 , 0.47280636],\n",
       "       [0.48781154, 0.58276576, 0.5955215 , 0.26253605, 0.23403965,\n",
       "        0.3075713 , 0.6323519 , 0.23417795, 0.31943724, 0.5318687 ,\n",
       "        0.54382765, 0.48182756, 0.36900467, 0.65713537, 0.46129164,\n",
       "        0.29567474, 0.5779099 , 0.3170758 , 0.5024574 , 0.46808827,\n",
       "        0.51134366, 0.69557774, 0.6384185 , 0.53739065, 0.48069414,\n",
       "        0.48174188, 0.5561918 , 0.29993996, 0.33696616, 0.47378758,\n",
       "        0.5670655 , 0.50516516],\n",
       "       [0.44109115, 0.61989814, 0.46846992, 0.35754323, 0.48407012,\n",
       "        0.40270922, 0.35309365, 0.34260908, 0.4034252 , 0.36463565,\n",
       "        0.69814545, 0.6392343 , 0.57531947, 0.46939135, 0.3846594 ,\n",
       "        0.40886348, 0.62693185, 0.580384  , 0.5755898 , 0.5631238 ,\n",
       "        0.6814403 , 0.5215402 , 0.68350655, 0.4889651 , 0.4445437 ,\n",
       "        0.5579929 , 0.7456913 , 0.59766686, 0.32870498, 0.51947457,\n",
       "        0.5714271 , 0.5880342 ],\n",
       "       [0.56518805, 0.38212547, 0.78028554, 0.43898773, 0.4776307 ,\n",
       "        0.67806387, 0.45792624, 0.3321926 , 0.4986086 , 0.39700133,\n",
       "        0.47602853, 0.6003727 , 0.4093992 , 0.66374326, 0.6144278 ,\n",
       "        0.42618734, 0.5735666 , 0.51491857, 0.5270694 , 0.50075257,\n",
       "        0.47055203, 0.64304715, 0.43239248, 0.62122124, 0.34238228,\n",
       "        0.5048617 , 0.5470597 , 0.5207674 , 0.546722  , 0.68913734,\n",
       "        0.6528392 , 0.31377694],\n",
       "       [0.52863586, 0.36258912, 0.43388075, 0.5073572 , 0.7030807 ,\n",
       "        0.6982256 , 0.36292255, 0.26242146, 0.5255009 , 0.4053931 ,\n",
       "        0.54677314, 0.796406  , 0.39071432, 0.445807  , 0.532732  ,\n",
       "        0.77553046, 0.3041465 , 0.5291772 , 0.71432763, 0.53973114,\n",
       "        0.35209933, 0.7437051 , 0.81391084, 0.2628241 , 0.503256  ,\n",
       "        0.49123386, 0.68551975, 0.39307538, 0.4992806 , 0.6926478 ,\n",
       "        0.41880262, 0.5272344 ],\n",
       "       [0.6627947 , 0.32501572, 0.42008978, 0.62974626, 0.41890392,\n",
       "        0.24313731, 0.7272358 , 0.4951741 , 0.6083384 , 0.7022502 ,\n",
       "        0.34264794, 0.42623204, 0.40234235, 0.29973558, 0.65960103,\n",
       "        0.650475  , 0.45714206, 0.34521067, 0.3698813 , 0.5983567 ,\n",
       "        0.5077806 , 0.58661383, 0.45442262, 0.6153463 , 0.3131071 ,\n",
       "        0.34430614, 0.6744941 , 0.5278022 , 0.5559432 , 0.4925209 ,\n",
       "        0.3296831 , 0.6293824 ]], dtype=float32)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 32), dtype=float32, numpy=\n",
       "array([[ 0.05627489,  0.0765749 , -0.17148174,  0.04548818,  0.03238086,\n",
       "         0.09054439,  0.29433888,  0.02722451, -0.10929022, -0.1042285 ,\n",
       "         0.10032048, -0.08055569,  0.182501  ,  0.03270156, -0.05111875,\n",
       "         0.15232173, -0.1829652 , -0.19298   ,  0.00699575, -0.2746634 ,\n",
       "         0.3781533 ,  0.10242426,  0.2531554 , -0.37442407,  0.16805875,\n",
       "         0.00163681,  0.15103143, -0.19702393, -0.10702232,  0.02750298,\n",
       "         0.01500319, -0.28751376],\n",
       "       [-0.08968791,  0.05153225, -0.14298752, -0.21957736, -0.10381615,\n",
       "        -0.12201183,  0.24632297,  0.20631096,  0.18002822, -0.22806312,\n",
       "        -0.22915427, -0.18283159,  0.20275609, -0.06353931,  0.22910093,\n",
       "        -0.12281308, -0.24024928, -0.09029342, -0.2840215 , -0.06294989,\n",
       "         0.04115437,  0.29820126, -0.37069237,  0.04251494,  0.26545495,\n",
       "         0.22440068, -0.01250129, -0.25229958, -0.11035067,  0.33077276,\n",
       "         0.03193088, -0.36009035],\n",
       "       [-0.1276256 , -0.07280283, -0.11381584,  0.00107119, -0.14089519,\n",
       "         0.00254467,  0.02095421, -0.0682554 ,  0.16984828, -0.08830623,\n",
       "         0.1135776 , -0.0359749 , -0.2092504 ,  0.2660045 , -0.00322721,\n",
       "        -0.21710038, -0.08408884,  0.04719041, -0.06746338,  0.180159  ,\n",
       "         0.08392006,  0.21798722,  0.06869366, -0.09457318, -0.11763052,\n",
       "         0.04807172,  0.35586074, -0.17111675,  0.01836485, -0.13447243,\n",
       "         0.06494617,  0.030736  ],\n",
       "       [ 0.00701893,  0.30357218,  0.08079637, -0.01740109, -0.09017303,\n",
       "         0.053227  , -0.06579452, -0.16876622,  0.07549007,  0.06092788,\n",
       "        -0.05697545,  0.11850137,  0.00606135,  0.32443592, -0.00723593,\n",
       "        -0.08414122, -0.16523296,  0.0754696 ,  0.28728417,  0.02327794,\n",
       "        -0.1732048 , -0.04886481, -0.12883084,  0.13423689, -0.05773997,\n",
       "         0.45293623,  0.10038297, -0.1273841 ,  0.12017721, -0.09798515,\n",
       "        -0.154957  ,  0.20357406],\n",
       "       [ 0.0791555 ,  0.14680302,  0.08578868, -0.06588076,  0.20652187,\n",
       "        -0.4308626 ,  0.20959142, -0.11785713,  0.02982317, -0.24624375,\n",
       "         0.07386566, -0.03087565,  0.13528305, -0.18722065, -0.19942978,\n",
       "        -0.14775175, -0.21294595, -0.20507902,  0.04540684, -0.38562512,\n",
       "        -0.0455691 , -0.13791758, -0.03758934,  0.3980194 ,  0.07625403,\n",
       "         0.1871787 ,  0.07013082,  0.08214244,  0.08872789,  0.45365092,\n",
       "         0.0516765 , -0.13220671],\n",
       "       [-0.00656059,  0.13560687,  0.2495565 , -0.06159608,  0.02742156,\n",
       "         0.06158647, -0.02935821,  0.02761724,  0.00396863,  0.11968904,\n",
       "        -0.01438095,  0.01324005,  0.00472841, -0.12084654, -0.00788415,\n",
       "         0.10452987, -0.011891  ,  0.05960535,  0.13974522, -0.2374412 ,\n",
       "         0.16618523, -0.14632502,  0.12474546, -0.20862663, -0.03591832,\n",
       "         0.30560875, -0.07121529, -0.07559677,  0.169251  , -0.2872982 ,\n",
       "         0.13403167, -0.0343725 ],\n",
       "       [ 0.01779761, -0.01593768, -0.14048508,  0.00776812,  0.16265221,\n",
       "         0.04840541,  0.16496749, -0.09330083,  0.09489877, -0.21927129,\n",
       "         0.09289438, -0.09303835,  0.27629834,  0.1682078 , -0.14194359,\n",
       "         0.33179545, -0.07783676,  0.02417515,  0.11375499, -0.22848204,\n",
       "         0.00464053,  0.07572449,  0.0375033 ,  0.1352274 ,  0.25585288,\n",
       "        -0.05505396,  0.05672624, -0.02553432, -0.01416661, -0.12017304,\n",
       "         0.06981297,  0.21381998],\n",
       "       [-0.19755808,  0.24113765,  0.20468304,  0.08779564, -0.05362836,\n",
       "         0.3403804 ,  0.12069684,  0.26383188,  0.04062821, -0.21539429,\n",
       "         0.10463993,  0.10554669, -0.5405546 , -0.10334668,  0.09517609,\n",
       "         0.12415478,  0.01665949, -0.12894587, -0.01073187, -0.10397343,\n",
       "        -0.04993476,  0.18987991,  0.49254736,  0.00900489, -0.11766488,\n",
       "         0.02330057, -0.14517966,  0.01374877, -0.00472792, -0.18670292,\n",
       "        -0.09777309, -0.00632147]], dtype=float32)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm2 = tf.keras.layers.LSTM(out_dim)\n",
    "output = lstm2(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.random.normal([32, 10, 8])\n",
    "lstm = tf.keras.layers.LSTM(4)\n",
    "output = lstm(inputs)\n",
    "print(output.shape)\n",
    "\n",
    "lstm = tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)\n",
    "whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)\n",
    "print(whole_seq_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import activations\n",
    "from keras import backend\n",
    "from keras import constraints\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras.engine.base_layer import Layer\n",
    "# from keras.engine.input_spec import InputSpec\n",
    "# from keras.saving.saved_model import layer_serialization\n",
    "# from keras.utils import control_flow_util\n",
    "# from keras.utils import generic_utils\n",
    "# from keras.utils import tf_utils\n",
    "# from tensorflow.python.platform import tf_logging as logging\n",
    "# from tensorflow.python.util.tf_export import keras_export\n",
    "# from tensorflow.tools.docs import doc_controls\n",
    "\n",
    "# Check the documentation for customized layer\n",
    "# https://keras.io/api/layers/base_layer/\n",
    "class LSTMCell(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "               units,\n",
    "               activation='tanh',\n",
    "               recurrent_activation='hard_sigmoid',\n",
    "               use_bias=True,\n",
    "               kernel_initializer='glorot_uniform',\n",
    "               recurrent_initializer='orthogonal',\n",
    "               bias_initializer='zeros',\n",
    "               unit_forget_bias=True,\n",
    "               kernel_regularizer=None,\n",
    "               recurrent_regularizer=None,\n",
    "               bias_regularizer=None,\n",
    "               kernel_constraint=None,\n",
    "               recurrent_constraint=None,\n",
    "               bias_constraint=None,\n",
    "               dropout=0.,\n",
    "               recurrent_dropout=0.,\n",
    "               **kwargs):\n",
    "        if units < 0:\n",
    "            raise ValueError(f'Received an invalid value for units, expected '\n",
    "                           f'a positive integer, got {units}.')\n",
    "        # By default use cached variable under v2 mode, see b/143699808.\n",
    "        if tf.compat.v1.executing_eagerly_outside_functions():\n",
    "            self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n",
    "        else:\n",
    "            self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n",
    "        super(LSTMCell, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.recurrent_activation = activations.get(recurrent_activation)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.unit_forget_bias = unit_forget_bias\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        self.dropout = min(1., max(0., dropout))\n",
    "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
    "        implementation = kwargs.pop('implementation', 1)\n",
    "        if self.recurrent_dropout != 0 and implementation != 1:\n",
    "            logging.debug(RECURRENT_DROPOUT_WARNING_MSG)\n",
    "            self.implementation = 1\n",
    "        else:\n",
    "            self.implementation = implementation\n",
    "        self.state_size = [self.units, self.units]\n",
    "        self.output_size = self.units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Required to add weights that are input shape dependent\n",
    "        \"\"\"\n",
    "        default_caching_device = _caching_device(self)\n",
    "        input_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_dim, self.units * 4),\n",
    "            name='kernel',\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            caching_device=default_caching_device)\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units * 4),\n",
    "            name='recurrent_kernel',\n",
    "            initializer=self.recurrent_initializer,\n",
    "            regularizer=self.recurrent_regularizer,\n",
    "            constraint=self.recurrent_constraint,\n",
    "            caching_device=default_caching_device)\n",
    "        if self.use_bias:\n",
    "            if self.unit_forget_bias:\n",
    "\n",
    "                def bias_initializer(_, *args, **kwargs):\n",
    "                    return backend.concatenate([\n",
    "                      self.bias_initializer((self.units,), *args, **kwargs),\n",
    "                      initializers.get('ones')((self.units,), *args, **kwargs),\n",
    "                      self.bias_initializer((self.units * 2,), *args, **kwargs),\n",
    "                  ])\n",
    "            else:\n",
    "                bias_initializer = self.bias_initializer\n",
    "            self.bias = self.add_weight(\n",
    "                          shape=(self.units * 4,),\n",
    "                          name='bias',\n",
    "                          initializer=bias_initializer,\n",
    "                          regularizer=self.bias_regularizer,\n",
    "                          constraint=self.bias_constraint,\n",
    "                          caching_device=default_caching_device)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.built = True\n",
    "\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, states, training=None):\n",
    "        h_tm1 = states[0]  # previous memory state\n",
    "        c_tm1 = states[1]  # previous carry state\n",
    "\n",
    "#         dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\n",
    "#         rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n",
    "#             h_tm1, training, count=4)\n",
    "\n",
    "        if self.implementation == 1:\n",
    "            if 0 < self.dropout < 1.:\n",
    "                inputs_i = inputs * dp_mask[0]\n",
    "                inputs_f = inputs * dp_mask[1]\n",
    "                inputs_c = inputs * dp_mask[2]\n",
    "                inputs_o = inputs * dp_mask[3]\n",
    "            else:\n",
    "                inputs_i = inputs\n",
    "                inputs_f = inputs\n",
    "                inputs_c = inputs\n",
    "                inputs_o = inputs\n",
    "            k_i, k_f, k_c, k_o = tf.split(\n",
    "              self.kernel, num_or_size_splits=4, axis=1)\n",
    "            x_i = backend.dot(inputs_i, k_i)\n",
    "            x_f = backend.dot(inputs_f, k_f)\n",
    "            x_c = backend.dot(inputs_c, k_c)\n",
    "            x_o = backend.dot(inputs_o, k_o)\n",
    "            if self.use_bias:\n",
    "                b_i, b_f, b_c, b_o = tf.split(\n",
    "                    self.bias, num_or_size_splits=4, axis=0)\n",
    "                x_i = backend.bias_add(x_i, b_i)\n",
    "                x_f = backend.bias_add(x_f, b_f)\n",
    "                x_c = backend.bias_add(x_c, b_c)\n",
    "                x_o = backend.bias_add(x_o, b_o)\n",
    "\n",
    "            if 0 < self.recurrent_dropout < 1.:\n",
    "                h_tm1_i = h_tm1 * rec_dp_mask[0]\n",
    "                h_tm1_f = h_tm1 * rec_dp_mask[1]\n",
    "                h_tm1_c = h_tm1 * rec_dp_mask[2]\n",
    "                h_tm1_o = h_tm1 * rec_dp_mask[3]\n",
    "            else:\n",
    "                h_tm1_i = h_tm1\n",
    "                h_tm1_f = h_tm1\n",
    "                h_tm1_c = h_tm1\n",
    "                h_tm1_o = h_tm1\n",
    "            x = (x_i, x_f, x_c, x_o)\n",
    "            h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o)\n",
    "            c, o = self._compute_carry_and_output(x, h_tm1, c_tm1)\n",
    "        else:\n",
    "            if 0. < self.dropout < 1.:\n",
    "                inputs = inputs * dp_mask[0]\n",
    "            z = backend.dot(inputs, self.kernel)\n",
    "            z += backend.dot(h_tm1, self.recurrent_kernel)\n",
    "            if self.use_bias:\n",
    "                z = backend.bias_add(z, self.bias)\n",
    "\n",
    "            z = tf.split(z, num_or_size_splits=4, axis=1)\n",
    "            c, o = self._compute_carry_and_output_fused(z, c_tm1)\n",
    "\n",
    "        h = o * self.activation(c)\n",
    "        return h, [h, c]\n",
    "\n",
    "    def _compute_carry_and_output(self, x, h_tm1, c_tm1):\n",
    "        \"\"\"Computes carry and output using split kernels.\"\"\"\n",
    "        x_i, x_f, x_c, x_o = x\n",
    "        h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o = h_tm1\n",
    "        i = self.recurrent_activation(\n",
    "            x_i + backend.dot(h_tm1_i, self.recurrent_kernel[:, :self.units]))\n",
    "        f = self.recurrent_activation(x_f + backend.dot(\n",
    "            h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]))\n",
    "        c = f * c_tm1 + i * self.activation(x_c + backend.dot(\n",
    "            h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))\n",
    "        o = self.recurrent_activation(\n",
    "            x_o + backend.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:]))\n",
    "        return c, o\n",
    "\n",
    "    \n",
    "def _caching_device(rnn_cell):\n",
    "    \"\"\"Returns the caching device for the RNN variable.\n",
    "    This is useful for distributed training, when variable is not located as same\n",
    "    device as the training worker. By enabling the device cache, this allows\n",
    "    worker to read the variable once and cache locally, rather than read it every\n",
    "    time step from remote when it is needed.\n",
    "    Note that this is assuming the variable that cell needs for each time step is\n",
    "    having the same value in the forward path, and only gets updated in the\n",
    "    backprop. It is true for all the default cells (SimpleRNN, GRU, LSTM). If the\n",
    "    cell body relies on any variable that gets updated every time step, then\n",
    "    caching device will cause it to read the stall value.\n",
    "    Args:\n",
    "    rnn_cell: the rnn cell instance.\n",
    "    \"\"\"\n",
    "    if tf.executing_eagerly():\n",
    "        # caching_device is not supported in eager mode.\n",
    "        return None\n",
    "    if not getattr(rnn_cell, '_enable_caching_device', False):\n",
    "        return None\n",
    "    # Don't set a caching device when running in a loop, since it is possible that\n",
    "    # train steps could be wrapped in a tf.while_loop. In that scenario caching\n",
    "    # prevents forward computations in loop iterations from re-reading the\n",
    "    # updated weights.\n",
    "    if control_flow_util.IsInWhileLoop(tf.compat.v1.get_default_graph()):\n",
    "        logging.warning(\n",
    "            'Variable read device caching has been disabled because the '\n",
    "            'RNN is in tf.while_loop loop context, which will cause '\n",
    "            'reading stalled value in forward path. This could slow down '\n",
    "            'the training due to duplicated variable reads. Please '\n",
    "            'consider updating your code to remove tf.while_loop if possible.')\n",
    "        return None\n",
    "    if (rnn_cell._dtype_policy.compute_dtype !=\n",
    "        rnn_cell._dtype_policy.variable_dtype):\n",
    "        logging.warning(\n",
    "            'Variable read device caching has been disabled since it '\n",
    "            'doesn\\'t work with the mixed precision API. This is '\n",
    "            'likely to cause a slowdown for RNN training due to '\n",
    "            'duplicated read of variable for each timestep, which '\n",
    "            'will be significant in a multi remote worker setting. '\n",
    "            'Please consider disabling mixed precision API if '\n",
    "            'the performance has been affected.')\n",
    "        return None\n",
    "    # Cache the value on the device that access the variable.\n",
    "    return lambda op: op.device\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 8), dtype=float32, numpy=\n",
       " array([[-0.10476895,  0.01909216,  0.04259723,  0.22393304, -0.06510702,\n",
       "          0.0607237 , -0.10680267, -0.3796199 ],\n",
       "        [ 0.        ,  0.03360127,  0.2594817 ,  0.06467079,  0.        ,\n",
       "          0.58613276, -0.02267295, -0.760425  ],\n",
       "        [ 0.        ,  0.        ,  0.21239248,  0.        ,  0.        ,\n",
       "          0.7520532 , -0.        , -0.7615706 ]], dtype=float32)>,\n",
       " [<tf.Tensor: shape=(3, 8), dtype=float32, numpy=\n",
       "  array([[-0.10476895,  0.01909216,  0.04259723,  0.22393304, -0.06510702,\n",
       "           0.0607237 , -0.10680267, -0.3796199 ],\n",
       "         [ 0.        ,  0.03360127,  0.2594817 ,  0.06467079,  0.        ,\n",
       "           0.58613276, -0.02267295, -0.760425  ],\n",
       "         [ 0.        ,  0.        ,  0.21239248,  0.        ,  0.        ,\n",
       "           0.7520532 , -0.        , -0.7615706 ]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(3, 8), dtype=float32, numpy=\n",
       "  array([[-0.22255842,  0.0431564 ,  0.08070901,  0.68405485, -0.12038146,\n",
       "           0.0903467 , -0.45964837, -0.68456656],\n",
       "         [ 0.        ,  0.36401477,  0.75201327,  0.9661032 ,  0.        ,\n",
       "           0.67175454, -0.3020275 , -0.997222  ],\n",
       "         [ 0.        ,  0.4051124 ,  0.9528622 ,  1.        ,  0.        ,\n",
       "           0.97766465, -0.09346282, -0.999944  ]], dtype=float32)>])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "sentence_max_length = 5\n",
    "n_features = 4\n",
    "out_features = 8\n",
    "new_shape = (batch_size, sentence_max_length, n_features)\n",
    "# x = tf.constant(np.reshape(np.arange(30), new_shape), dtype = tf.float32)\n",
    "\n",
    "x = tf.constant(np.reshape(np.arange(batch_size * n_features), [batch_size, n_features]), dtype = tf.float32)\n",
    "states = [tf.zeros([batch_size, out_features]), tf.zeros([batch_size, out_features])]\n",
    "\n",
    "model = LSTMCell(out_features)\n",
    "model(x, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Sequential Model Using the Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5, 8), dtype=float32, numpy=\n",
       "array([[[-0.3685529 ,  0.03548042, -0.24419327, -0.2727273 ,\n",
       "         -0.0813837 ,  0.0989939 , -0.11930474, -0.20282263],\n",
       "        [-0.7565586 , -0.05964788, -0.45405662, -0.51776266,\n",
       "         -0.58944976,  0.14515461, -0.151285  , -0.52873456],\n",
       "        [-0.7174803 ,  0.02329051, -0.41103694, -0.5624658 ,\n",
       "         -0.72346914,  0.06634909, -0.11814035, -0.56606746],\n",
       "        [-0.6727449 ,  0.15986285, -0.3403043 , -0.49626243,\n",
       "         -0.65444446,  0.11354627, -0.08819287, -0.56675744],\n",
       "        [-0.6417773 ,  0.24228156, -0.27740368, -0.4014975 ,\n",
       "         -0.56822044,  0.1695401 , -0.09530976, -0.53987384]],\n",
       "\n",
       "       [[-0.5910057 ,  0.26778895, -0.21793066, -0.30680513,\n",
       "         -0.5112453 ,  0.23893647, -0.0790079 , -0.5313454 ],\n",
       "        [-0.56336236,  0.31183356, -0.18396243, -0.2624165 ,\n",
       "         -0.5085331 ,  0.27645475, -0.05093649, -0.5521839 ],\n",
       "        [-0.54038346,  0.332192  , -0.15500873, -0.20631021,\n",
       "         -0.48263872,  0.31286177, -0.04272832, -0.5481477 ],\n",
       "        [-0.5487292 ,  0.31519595, -0.15459259, -0.1838809 ,\n",
       "         -0.45738864,  0.31858078, -0.06955831, -0.52403486],\n",
       "        [-0.55668426,  0.299772  , -0.15424459, -0.16373986,\n",
       "         -0.43461618,  0.32369038, -0.09432276, -0.5022373 ]],\n",
       "\n",
       "       [[-0.560007  ,  0.27976257, -0.15324861, -0.13369037,\n",
       "         -0.40246636,  0.33179414, -0.11881226, -0.47240242],\n",
       "        [-0.5727126 ,  0.26862815, -0.15353924, -0.12305211,\n",
       "         -0.38862154,  0.33401495, -0.14428918, -0.4582163 ],\n",
       "        [-0.5812031 ,  0.25247854, -0.15318741, -0.10205384,\n",
       "         -0.3648378 ,  0.33933103, -0.17039464, -0.43542913],\n",
       "        [-0.5856117 ,  0.24390374, -0.15299286, -0.0908488 ,\n",
       "         -0.35217246,  0.34217462, -0.18414697, -0.42330784],\n",
       "        [-0.58573264,  0.24395807, -0.15300567, -0.09100319,\n",
       "         -0.35230815,  0.34212524, -0.18422227, -0.42341778]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import RNN\n",
    "batch_size = 3\n",
    "seq_len = 5\n",
    "inp_dim = 4\n",
    "out_dim = 8\n",
    "\n",
    "lstm_layer = tf.keras.layers.RNN(LSTMCell(out_features),\n",
    "                                 input_shape=(None, n_features),\n",
    "                                 return_sequences=True,\n",
    "                                 return_state=False\n",
    "                                )\n",
    "model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            lstm_layer,\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(out_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "new_shape = (batch_size, seq_len, inp_dim)\n",
    "x = tf.constant(np.reshape(np.arange(60), new_shape), dtype = tf.float32)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A General Model Using the Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5, 8), dtype=float32, numpy=\n",
       "array([[[ 0.05392185,  0.11244936,  0.0817205 , -0.2527224 ,\n",
       "          0.09619954,  0.00545769, -0.20760998, -0.1781682 ],\n",
       "        [-0.01148473,  0.26539844,  0.08757333, -0.3929581 ,\n",
       "          0.12792118,  0.12631851, -0.19791119, -0.25857073],\n",
       "        [-0.03495827,  0.25147653,  0.08493275, -0.41013473,\n",
       "          0.09135444,  0.10869552, -0.17762847, -0.24655099],\n",
       "        [-0.04157254,  0.2296985 ,  0.07936808, -0.39513916,\n",
       "          0.0662499 ,  0.09229817, -0.15479581, -0.22584179],\n",
       "        [-0.04791046,  0.20781992,  0.07372073, -0.37942111,\n",
       "          0.04157673,  0.07604742, -0.13209486, -0.20501654]],\n",
       "\n",
       "       [[-0.05181512,  0.1892882 ,  0.06866545, -0.362992  ,\n",
       "          0.02328804,  0.06334292, -0.11399756, -0.18727951],\n",
       "        [-0.0605114 ,  0.16408485,  0.06241901, -0.3478551 ,\n",
       "         -0.00762257,  0.04361196, -0.08676881, -0.16338247],\n",
       "        [-0.0668653 ,  0.1421248 ,  0.05674921, -0.33206227,\n",
       "         -0.03237402,  0.02730623, -0.06398922, -0.14247917],\n",
       "        [-0.07318348,  0.12022462,  0.05109145, -0.31627342,\n",
       "         -0.05702523,  0.01105826, -0.04128593, -0.12163161],\n",
       "        [-0.07950117,  0.0983253 ,  0.04543388, -0.3004846 ,\n",
       "         -0.081675  , -0.00518888, -0.01858374, -0.10078486]],\n",
       "\n",
       "       [[-0.08332615,  0.08008657,  0.04045518, -0.284277  ,\n",
       "         -0.09964254, -0.01767953, -0.00078642, -0.08332705],\n",
       "        [-0.09209985,  0.05458882,  0.03413129, -0.26891187,\n",
       "         -0.13087115, -0.03762347,  0.02674172, -0.05914927],\n",
       "        [-0.09382549,  0.04986595,  0.0329786 , -0.2662798 ,\n",
       "         -0.13683474, -0.04139043,  0.03191834, -0.05467759],\n",
       "        [-0.09421326,  0.05007204,  0.0331149 , -0.2673803 ,\n",
       "         -0.13740027, -0.04156149,  0.03205025, -0.05490357],\n",
       "        [-0.09460214,  0.05027872,  0.03325158, -0.26848394,\n",
       "         -0.13796741, -0.04173304,  0.03218254, -0.05513019]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class spLSTM(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(spLSTM, self).__init__()\n",
    "\n",
    "        self.input_dim = kwargs.get(\"input_dim\", None)\n",
    "        self.out_features = kwargs.get(\"out_dim\", None)\n",
    "        \n",
    "        self.cell = LSTMCell(out_features)\n",
    "        self.lstm_layer = tf.keras.layers.RNN(self.cell,\n",
    "                                              input_shape=(None, self.input_dim),\n",
    "                                              return_sequences=True,\n",
    "                                              return_state=False\n",
    "                                             )\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "        self.linear = tf.keras.layers.Dense(self.out_features)\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        y = self.lstm_layer(x)\n",
    "        y = self.bn(y)\n",
    "        y = self.linear(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "new_shape = (batch_size, seq_len, inp_dim)\n",
    "x = tf.constant(np.reshape(np.arange(60), new_shape), dtype = tf.float32)\n",
    "model = spLSTM(input_dim=inp_dim, out_dim=out_dim)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Aware LSTM Cell\n",
    "\n",
    "Based on Yu et al., 2019, Adaptive User Modeling with Long and Short Term Preferences for Personalized Recommendation. \n",
    "\n",
    "Original LSTM equations are modified as follows: \n",
    "\n",
    "$$c_k = f_k \\odot c_{k-1} + i_k \\odot \\phi(x_kW_c + h_{k-1}U_c + b_c)$$\n",
    "$$c_k = f_k \\odot T_\\delta \\odot c_{k-1} + i_k \\odot T_s \\odot \\phi(x_kW_c + h_{k-1}U_c + b_c)$$\n",
    "\n",
    "$$o_k = \\sigma \\left(x_kW_o + h_{k-1}U_o + b_o \\right)$$\n",
    "$$o_k = \\sigma \\left(x_kW_o + \\underbrace{\\delta_{tk}W_{\\delta o} + s_{tk}W_{so}}  + h_{k-1}U_o + b_o \\right)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\delta_{tk} = \\phi\\left( W_{\\delta}\\log(t_k - t_{k-1}) + b_{\\delta} \\right) $$\n",
    "$$s_{tk} = \\phi\\left( W_s\\log(t_p - t_k) + b_s \\right) $$\n",
    "$$T_\\delta = \\sigma \\left( x_kW_{x\\delta} + \\delta_{tk}W_{t\\delta} + b_{t\\delta}\\right)$$\n",
    "$$T_s = \\sigma \\left( x_kW_{xs} + s_{tk}W_{ts} + b_{ts}\\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TALSTMCell(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Time Aware LSTM Cell\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "               units,\n",
    "               activation='tanh',\n",
    "               recurrent_activation='hard_sigmoid',\n",
    "               use_bias=True,\n",
    "               kernel_initializer='glorot_uniform',\n",
    "               recurrent_initializer='orthogonal',\n",
    "               bias_initializer='zeros',\n",
    "               unit_forget_bias=True,\n",
    "               kernel_regularizer=None,\n",
    "               recurrent_regularizer=None,\n",
    "               bias_regularizer=None,\n",
    "               kernel_constraint=None,\n",
    "               recurrent_constraint=None,\n",
    "               bias_constraint=None,\n",
    "               dropout=0.,\n",
    "               recurrent_dropout=0.,\n",
    "               **kwargs):\n",
    "        if units < 0:\n",
    "            raise ValueError(f'Received an invalid value for units, expected '\n",
    "                           f'a positive integer, got {units}.')\n",
    "        # By default use cached variable under v2 mode, see b/143699808.\n",
    "        if tf.compat.v1.executing_eagerly_outside_functions():\n",
    "            self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n",
    "        else:\n",
    "            self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n",
    "        super(TALSTMCell, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.recurrent_activation = activations.get(recurrent_activation)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.unit_forget_bias = unit_forget_bias\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        self.dropout = min(1., max(0., dropout))\n",
    "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
    "        implementation = kwargs.pop('implementation', 1)\n",
    "        if self.recurrent_dropout != 0 and implementation != 1:\n",
    "            logging.debug(RECURRENT_DROPOUT_WARNING_MSG)\n",
    "            self.implementation = 1\n",
    "        else:\n",
    "            self.implementation = implementation\n",
    "        self.state_size = [self.units, self.units]\n",
    "        self.output_size = self.units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Required to add weights that are input shape dependent\n",
    "        \"\"\"\n",
    "        default_caching_device = _caching_device(self)\n",
    "        input_dim = input_shape[-1]-2\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_dim, self.units * 4),\n",
    "            name='kernel',\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            caching_device=default_caching_device)\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units * 4),\n",
    "            name='recurrent_kernel',\n",
    "            initializer=self.recurrent_initializer,\n",
    "            regularizer=self.recurrent_regularizer,\n",
    "            constraint=self.recurrent_constraint,\n",
    "            caching_device=default_caching_device)\n",
    "        # time related\n",
    "        # W_xdelta, W_tdelta, W_xs and W_ts, Eqs. (9) & (10)\n",
    "        self.time_kernel = self.add_weight(\n",
    "            shape=(input_dim, self.units * 4),\n",
    "            name='time_kernel',\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            caching_device=default_caching_device    \n",
    "        )\n",
    "        \n",
    "        # W_delta and W_s for Eqs.(7) & (8)\n",
    "        self.time_kernel2 = self.add_weight(\n",
    "            shape=(2, input_dim),\n",
    "            name='time_kernel2',\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            caching_device=default_caching_device    \n",
    "        )\n",
    "        \n",
    "        # W_delta,o and W_so for Eq.(12)\n",
    "        self.time_kernel3 = self.add_weight(\n",
    "            shape=(input_dim, self.units * 2),\n",
    "            name='time_kernel3',\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            caching_device=default_caching_device    \n",
    "        )\n",
    "        if self.use_bias:\n",
    "            if self.unit_forget_bias:\n",
    "\n",
    "                def bias_initializer(_, *args, **kwargs):\n",
    "                    return backend.concatenate([\n",
    "                      self.bias_initializer((self.units,), *args, **kwargs),\n",
    "                      initializers.get('ones')((self.units,), *args, **kwargs),\n",
    "                      self.bias_initializer((self.units * 2,), *args, **kwargs),\n",
    "                  ])\n",
    "            else:\n",
    "                bias_initializer = self.bias_initializer\n",
    "                \n",
    "            self.bias = self.add_weight(\n",
    "                          shape=(self.units * 4,),\n",
    "                          name='bias',\n",
    "                          initializer=bias_initializer,\n",
    "                          regularizer=self.bias_regularizer,\n",
    "                          constraint=self.bias_constraint,\n",
    "                          caching_device=default_caching_device)\n",
    "            # time related\n",
    "            self.time_bias1 = self.add_weight(\n",
    "                          shape=(input_dim * 2,),\n",
    "                          name='time_bias1',\n",
    "                          initializer=self.bias_initializer,\n",
    "                          regularizer=self.bias_regularizer,\n",
    "                          constraint=self.bias_constraint,\n",
    "                          caching_device=default_caching_device)\n",
    "            self.time_bias2 = self.add_weight(\n",
    "                          shape=(self.units * 2,),\n",
    "                          name='time_bias2',\n",
    "                          initializer=self.bias_initializer,\n",
    "                          regularizer=self.bias_regularizer,\n",
    "                          constraint=self.bias_constraint,\n",
    "                          caching_device=default_caching_device)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.built = True\n",
    "\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        h_tm1 = states[0]  # previous memory state\n",
    "        c_tm1 = states[1]  # previous carry state\n",
    "\n",
    "#         dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\n",
    "#         rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n",
    "#             h_tm1, training, count=4)\n",
    "\n",
    "        # take out the time features\n",
    "        tf1 = tf.expand_dims(inputs[:, -1], -1)\n",
    "        tf2 = tf.expand_dims(inputs[:, -2], -1)\n",
    "        inputs = inputs[:, :-2]\n",
    "\n",
    "        if self.implementation == 1:\n",
    "            if 0 < self.dropout < 1.:\n",
    "                inputs_i = inputs * dp_mask[0]\n",
    "                inputs_f = inputs * dp_mask[1]\n",
    "                inputs_c = inputs * dp_mask[2]\n",
    "                inputs_o = inputs * dp_mask[3]\n",
    "            else:\n",
    "                inputs_i = inputs\n",
    "                inputs_f = inputs\n",
    "                inputs_c = inputs\n",
    "                inputs_o = inputs\n",
    "                inputs_d = inputs # delta\n",
    "                inputs_s = inputs # s\n",
    "            k_i, k_f, k_c, k_o = tf.split(\n",
    "              self.kernel, num_or_size_splits=4, axis=1)\n",
    "            W_xd, W_xs, W_td, W_ts = tf.split(\n",
    "              self.time_kernel, num_or_size_splits=4, axis=1)\n",
    "            W_d, W_s = tf.split(\n",
    "              self.time_kernel2, num_or_size_splits=2, axis=0)\n",
    "            x_i = backend.dot(inputs_i, k_i)\n",
    "            x_f = backend.dot(inputs_f, k_f)\n",
    "            x_c = backend.dot(inputs_c, k_c)\n",
    "            x_o = backend.dot(inputs_o, k_o)\n",
    "            # time related\n",
    "            delta_tk = backend.dot(tf1, W_d)\n",
    "            s_tk = backend.dot(tf2, W_s)\n",
    "            T_d = backend.dot(inputs_d, W_xd)  # T_delta\n",
    "            T_s = backend.dot(inputs_s, W_xs)  # T_s\n",
    "            if self.use_bias:\n",
    "                b_i, b_f, b_c, b_o = tf.split(\n",
    "                    self.bias, num_or_size_splits=4, axis=0)\n",
    "                \n",
    "                x_i = backend.bias_add(x_i, b_i)\n",
    "                x_f = backend.bias_add(x_f, b_f)\n",
    "                x_c = backend.bias_add(x_c, b_c)\n",
    "                x_o = backend.bias_add(x_o, b_o)\n",
    "\n",
    "                b_d, b_s = tf.split(\n",
    "                    self.time_bias1, num_or_size_splits=2, axis=0)\n",
    "                delta_tk = backend.bias_add(delta_tk, b_d)\n",
    "                s_tk = backend.bias_add(s_tk, b_s)                \n",
    "                \n",
    "                b_td, b_ts = tf.split(\n",
    "                    self.time_bias2, num_or_size_splits=2, axis=0)    \n",
    "                T_d = backend.bias_add(T_d, b_td)\n",
    "                T_s = backend.bias_add(T_s, b_ts)\n",
    "\n",
    "            delta_tk = self.activation(delta_tk)  # complete\n",
    "            s_tk = self.activation(s_tk)  # complete\n",
    "\n",
    "            T_d = T_d + backend.dot(delta_tk, W_td)\n",
    "            T_s = T_s + backend.dot(s_tk, W_ts)\n",
    "            T_delta = self.recurrent_activation(T_d)\n",
    "            T_s = self.recurrent_activation(T_s)\n",
    "            \n",
    "            # Eq.(12)\n",
    "            W_do, W_so = tf.split(\n",
    "              self.time_kernel3, num_or_size_splits=2, axis=1)\n",
    "            x_o = x_o + backend.dot(delta_tk, W_do)\n",
    "            x_o = x_o + backend.dot(s_tk, W_so)\n",
    "\n",
    "            if 0 < self.recurrent_dropout < 1.:\n",
    "                h_tm1_i = h_tm1 * rec_dp_mask[0]\n",
    "                h_tm1_f = h_tm1 * rec_dp_mask[1]\n",
    "                h_tm1_c = h_tm1 * rec_dp_mask[2]\n",
    "                h_tm1_o = h_tm1 * rec_dp_mask[3]\n",
    "            else:\n",
    "                h_tm1_i = h_tm1\n",
    "                h_tm1_f = h_tm1\n",
    "                h_tm1_c = h_tm1\n",
    "                h_tm1_o = h_tm1\n",
    "            x = (x_i, x_f, x_c, x_o)\n",
    "            h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o)\n",
    "            c, o = self._compute_carry_and_output(x, h_tm1, c_tm1, T_delta, T_s)\n",
    "        else:\n",
    "            if 0. < self.dropout < 1.:\n",
    "                inputs = inputs * dp_mask[0]\n",
    "            z = backend.dot(inputs, self.kernel)\n",
    "            z += backend.dot(h_tm1, self.recurrent_kernel)\n",
    "            if self.use_bias:\n",
    "                z = backend.bias_add(z, self.bias)\n",
    "\n",
    "            z = tf.split(z, num_or_size_splits=4, axis=1)\n",
    "            c, o = self._compute_carry_and_output_fused(z, c_tm1)\n",
    "\n",
    "        h = o * self.activation(c)\n",
    "        return h, [h, c]\n",
    "\n",
    "    def _compute_carry_and_output(self, x, h_tm1, c_tm1, T_delta, T_s):\n",
    "        \"\"\"Computes carry and output using split kernels.\"\"\"\n",
    "        x_i, x_f, x_c, x_o = x\n",
    "        h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o = h_tm1\n",
    "        i = self.recurrent_activation(\n",
    "            x_i + backend.dot(h_tm1_i, self.recurrent_kernel[:, :self.units]))\n",
    "        f = self.recurrent_activation(x_f + backend.dot(\n",
    "            h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]))\n",
    "        c = f * T_delta * c_tm1 + i * T_s * self.activation(x_c + backend.dot(\n",
    "            h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))\n",
    "        o = self.recurrent_activation(\n",
    "            x_o + backend.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:]))\n",
    "        return c, o\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 8), dtype=float32, numpy=\n",
       " array([[ 0.01555797,  0.10276201,  0.00688872, -0.03902443, -0.00844985,\n",
       "         -0.05784162,  0.04423954, -0.01630299],\n",
       "        [ 0.05028533,  0.2974947 ,  0.        , -0.00615981,  0.        ,\n",
       "          0.02751383, -0.02384676,  0.        ],\n",
       "        [ 0.00584659,  0.27857697,  0.        , -0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ]], dtype=float32)>,\n",
       " [<tf.Tensor: shape=(3, 8), dtype=float32, numpy=\n",
       "  array([[ 0.01555797,  0.10276201,  0.00688872, -0.03902443, -0.00844985,\n",
       "          -0.05784162,  0.04423954, -0.01630299],\n",
       "         [ 0.05028533,  0.2974947 ,  0.        , -0.00615981,  0.        ,\n",
       "           0.02751383, -0.02384676,  0.        ],\n",
       "         [ 0.00584659,  0.27857697,  0.        , -0.        ,  0.        ,\n",
       "           0.        ,  0.        ,  0.        ]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(3, 8), dtype=float32, numpy=\n",
       "  array([[ 0.03604736,  0.1325443 ,  0.01069185, -0.08066487, -0.05353459,\n",
       "          -0.08607701,  0.055942  , -0.02632935],\n",
       "         [ 0.21569316,  0.3067688 ,  0.        , -0.09797075,  0.        ,\n",
       "           0.03396133, -0.02385129,  0.        ],\n",
       "         [ 0.08345944,  0.28613865,  0.        , -0.07577521,  0.        ,\n",
       "           0.        ,  0.        ,  0.        ]], dtype=float32)>])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "sentence_max_length = 5\n",
    "n_features = 4 + 2\n",
    "out_features = 8\n",
    "new_shape = (batch_size, sentence_max_length, n_features)\n",
    "# x = tf.constant(np.reshape(np.arange(30), new_shape), dtype = tf.float32)\n",
    "\n",
    "x = tf.constant(np.reshape(np.arange(batch_size * n_features), [batch_size, n_features]), dtype = tf.float32)\n",
    "states = [tf.zeros([batch_size, out_features]), tf.zeros([batch_size, out_features])]\n",
    "\n",
    "model = TALSTMCell(out_features)\n",
    "model(x, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 6), dtype=float32, numpy=\n",
       "array([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
       "       [ 6.,  7.,  8.,  9., 10., 11.],\n",
       "       [12., 13., 14., 15., 16., 17.]], dtype=float32)>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = (1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37_tensorflow]",
   "language": "python",
   "name": "conda-env-py37_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
